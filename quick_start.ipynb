{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5454d455",
   "metadata": {},
   "source": [
    "This is an examplar demonstration using CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c01e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import optim\n",
    "from CIFAR_dataset import *\n",
    "from models import *\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a067fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of datasets\n",
    "# Please modify accordingly\n",
    "data_root = \"../data\"\n",
    "\n",
    "batch_size = 1024\n",
    "zeta_trains = 1-np.array([0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2])\n",
    "latent_dim = 64\n",
    "num_epochs = 200\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "## Please select the task: \"origin\", \"regularization\", or \"groupDRO\"\n",
    "# task = \"origin\"\n",
    "# task = \"regularization\"\n",
    "task = \"groupDRO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19dfc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data\n",
    "trainset = torchvision.datasets.CIFAR10(data_root, train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ]))\n",
    "testset = torchvision.datasets.CIFAR10(data_root, train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                             ]))\n",
    "# Select the two binary classes from 0~9\n",
    "dig0,dig1 = 0, 1\n",
    "where_train = (torch.tensor(trainset.targets) == dig0) + (torch.tensor(trainset.targets) == dig1)\n",
    "where_test = (torch.tensor(testset.targets) == dig0) + (torch.tensor(testset.targets) == dig1)\n",
    "index_train = torch.where(where_train)[0]\n",
    "index_test = torch.where(where_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe4b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regularizaiton term\n",
    "def reg(emb, image, label, spurious):\n",
    "\n",
    "    mu_spur_1 = (emb[(spurious == 1)*(label == 1)].mean(0) + \\\n",
    "                 emb[(spurious == 1)*(label == 0)].mean(0))/2\n",
    "    mu_spur_0 = (emb[(spurious == 0)*(label == 1)].mean(0) + \\\n",
    "                 emb[(spurious == 0)*(label == 0)].mean(0))/2\n",
    "\n",
    "    mu_spur = ((mu_spur_1-mu_spur_0)/2).reshape(-1,1)\n",
    "    sig_spur = ((mu_spur_1 - emb[spurious == 1]).T.mm(\n",
    "        (mu_spur_1 - emb[spurious == 1])) + \\\n",
    "                    (mu_spur_0 - emb[spurious == 0]).T.mm(\n",
    "        (mu_spur_0 - emb[spurious == 0])))/(len(trainset_biased.data) - 1)\n",
    "\n",
    "    mu_info_1 = (emb[(spurious == 1)*(label == 1)].mean(0) + \\\n",
    "                 emb[(spurious == 0)*(label == 1)].mean(0))/2\n",
    "    mu_info_0 = (emb[(spurious == 1)*(label == 0)].mean(0) + \\\n",
    "                 emb[(spurious == 0)*(label == 0)].mean(0))/2\n",
    "\n",
    "    mu_info = ((mu_info_1-mu_info_0)/2).reshape(-1,1)\n",
    "\n",
    "    loss = mu_spur.norm()/mu_info.norm()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d359651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Train the models with no regularization or group DRO\n",
    "if task == \"origin\":\n",
    "    for seed in range(10):\n",
    "        print(f'==========================Seed: {seed}=============================')\n",
    "        for model_name in ['resnet', 'vgg', 'alexnet']:\n",
    "            print(f'========================Model: {model_name}=========================')\n",
    "            for zeta_train in zeta_trains:   \n",
    "                print(f'========================Zeta: {zeta_train}===========================')\n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "\n",
    "                trainset_nonbiased = CIFAR_subset_watermark(trainset, index_train, alpha = .5, beta = .5)\n",
    "                testset_nonbiased = CIFAR_subset_watermark(testset, index_test, alpha = .5, beta = .5)\n",
    "                trainset_biased = CIFAR_subset_watermark(trainset, index_train, alpha = zeta_train, beta = zeta_train)\n",
    "\n",
    "                trainloader = DataLoader(trainset_biased, batch_size=batch_size, num_workers = 8, shuffle=True)\n",
    "                Loss = []\n",
    "                if model_name == 'resnet':\n",
    "                    model = Classifier_resnet(latent_dim).to(device)\n",
    "                elif model_name == 'vgg':\n",
    "                    model = Classifier_vgg(latent_dim).to(device)\n",
    "                elif model_name == 'alexnet':\n",
    "                    model = Classifier_alexnet(latent_dim).to(device)\n",
    "                elif model_name == 'efficientnet':\n",
    "                    model = Classifier_efficientnet(latent_dim).to(device)\n",
    "                elif model_name == 'mobilenet':\n",
    "                    model = Classifier_mobilenet(latent_dim).to(device)\n",
    "\n",
    "                optimizer = optim.SGD(model.parameters(), lr = 1e-3, momentum = 0.9, weight_decay = 5e-4)\n",
    "                Acc_train = []\n",
    "                Acc_test = []\n",
    "                Cls_loss = []\n",
    "                Reg_loss = []\n",
    "                criterion = nn.BCELoss()\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    cls_loss = 0.0\n",
    "                    reg_loss = 0.0\n",
    "                    acc_train = 0\n",
    "                    model.train()\n",
    "                    for data in trainloader:\n",
    "                        images, label = data\n",
    "                        images = images.permute(0,3,1,2).to(device)\n",
    "                        label = label[:,0].unsqueeze(1).to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        emb, pred = model(images)\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            if ((data[1][:,1] == 1)*(data[1][:,0] == 0)).sum().item()*((data[1][:,1] == 0)*(data[1][:,0] == 1)).sum().item() == 0:\n",
    "                                loss_reg = 0\n",
    "                            else:\n",
    "                                loss_reg = reg(emb,images, data[1][:,0], data[1][:,1])\n",
    "\n",
    "                        loss_cls = criterion(pred, label)\n",
    "                        loss = loss_cls\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        cls_loss += loss_cls.item()\n",
    "                        if loss_reg > 0:\n",
    "                            reg_loss += loss_reg.item()\n",
    "                        acc_train += ((pred > 0.5)==label).float().sum().item()\n",
    "\n",
    "                    acc_train/=len(trainset_biased)\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        data = testset_nonbiased.data.permute(0,3,1,2).clone().detach().to(device)\n",
    "                        label = testset_nonbiased.targets.view(-1,1).clone().detach().to(device)\n",
    "                        _, pred = model(data)\n",
    "                        acc_test = ((pred > 0.5)==label).float().mean().item()\n",
    "\n",
    "                    if cls_loss/len(trainloader)>10:\n",
    "                        optimizer = optim.SGD(model.parameters(), lr = 1e-4, momentum = 0.9, weight_decay = 5e-4)\n",
    "\n",
    "                    print('S/M/Z: [%d/%s/%d], Epoch [%d/%d], Loss: [%.4f,%.4f], Acc: [%.4f/%.4f]' % (\n",
    "                        seed, model_name, zeta_train, epoch+1, num_epochs, cls_loss/len(trainloader),\n",
    "                        reg_loss/len(trainloader), acc_train, acc_test))\n",
    "\n",
    "                    Acc_test.append(acc_test)\n",
    "                    Acc_train.append(acc_train)\n",
    "                    Cls_loss.append(cls_loss/len(trainloader))\n",
    "                    Reg_loss.append(reg_loss/len(trainloader))\n",
    "\n",
    "\n",
    "                torch.save(model.state_dict(), f'results/cifar_watermark/noReg_{model_name}_{zeta_train}_f{num_epochs}_seed{seed}')\n",
    "                np.savetxt(f'results/cifar_watermark/trainAcc_zeta_{model_name}{zeta_train}_noReg_{num_epochs}_seed{seed}.txt', Acc_train)\n",
    "                np.savetxt(f'results/cifar_watermark/testAcc_zeta_{model_name}_{zeta_train}_noReg_{num_epochs}_seed{seed}.txt', Acc_test)\n",
    "                np.savetxt(f'results/cifar_watermark/clsLoss_zeta_{model_name}_{zeta_train}_noReg_{num_epochs}_seed{seed}.txt', Cls_loss)\n",
    "                np.savetxt(f'results/cifar_watermark/regLoss_zeta{model_name}_{zeta_train}_noReg_{num_epochs}_seed{seed}.txt', Reg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee691bee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if task == \"regularization\":\n",
    "    for seed in range(10):\n",
    "        print(f'==========================Seed: {seed}=============================')\n",
    "        for model_name in ['resnet', 'vgg', 'alexnet']:\n",
    "            print(f'========================Model: {model_name}=========================')\n",
    "            for zeta_train in zeta_trains:   \n",
    "                print(f'========================Zeta: {zeta_train}===========================')\n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "\n",
    "                testset_nonbiased = CIFAR_subset_watermark(testset, index_test, alpha = .5, beta = .5)\n",
    "                trainset_biased = CIFAR_subset_watermark(trainset, index_train, alpha = zeta_train, beta = zeta_train)\n",
    "\n",
    "                trainloader = DataLoader(trainset_biased, batch_size=batch_size, num_workers = 8, shuffle=True)\n",
    "                Loss = []\n",
    "                if model_name == 'resnet':\n",
    "                    model = Classifier_resnet(latent_dim).to(device)\n",
    "                elif model_name == 'vgg':\n",
    "                    model = Classifier_vgg(latent_dim).to(device)\n",
    "                elif model_name == 'alexnet':\n",
    "                    model = Classifier_alexnet(latent_dim).to(device)\n",
    "\n",
    "                optimizer = optim.SGD(model.parameters(), lr = 5e-4, momentum = 0.9, weight_decay = 1e-4)\n",
    "\n",
    "                Acc_train = []\n",
    "                Acc_test = []\n",
    "                Cls_loss = []\n",
    "                Reg_loss = []\n",
    "                criterion = nn.BCELoss()\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    cls_loss = 0.0\n",
    "                    reg_loss = 0.0\n",
    "                    acc_train = 0\n",
    "                    model.train()\n",
    "                    for idx, data in enumerate(trainloader):\n",
    "                        images, label = data\n",
    "                        images = images.permute(0,3,1,2).to(device)\n",
    "                        label = label[:,0].unsqueeze(1).to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        emb, pred = model(images)\n",
    "\n",
    "                        if ((data[1][:,1] == 1)*(data[1][:,0] == 0)).sum().item()*((data[1][:,1] == 0)*(data[1][:,0] == 1)).sum().item() == 0:\n",
    "                            loss_reg = 0\n",
    "                        else:\n",
    "                            loss_reg = reg(emb,images, data[1][:,0], data[1][:,1])\n",
    "\n",
    "                        loss_cls = criterion(pred, label)\n",
    "\n",
    "                        loss = loss_cls + loss_reg\n",
    "                        loss.backward()\n",
    "\n",
    "                        optimizer.step()\n",
    "                        cls_loss += loss_cls.item()\n",
    "                        if loss_reg > 0:\n",
    "                            reg_loss += loss_reg.item()\n",
    "                        acc_train += ((pred > 0.5)==label).float().sum().item()\n",
    "\n",
    "                    acc_train/=len(trainset_biased)\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        data = testset_nonbiased.data.permute(0,3,1,2).clone().detach().to(device)\n",
    "                        label = testset_nonbiased.targets.view(-1,1).clone().detach().to(device)\n",
    "                        _, pred = model(data)\n",
    "                        acc_test = ((pred > 0.5)==label).float().mean().item()\n",
    "\n",
    "                    print('Epoch [%d/%d], Loss: [%.4f,%.4f], Acc: [%.4f/%.4f]' % (\n",
    "                        epoch+1, num_epochs, cls_loss/len(trainloader),\n",
    "                        reg_loss/len(trainloader), acc_train, acc_test))\n",
    "\n",
    "                    Acc_test.append(acc_test)\n",
    "                    Acc_train.append(acc_train)\n",
    "                    Cls_loss.append(cls_loss/len(trainloader))\n",
    "                    Reg_loss.append(reg_loss/len(trainloader))\n",
    "\n",
    "\n",
    "                torch.save(model.state_dict(), f'results/cifar_watermark/Reg_{model_name}_{zeta_train}_{num_epochs}_seed{seed}')\n",
    "                np.savetxt(f'results/cifar_watermark/trainAcc_zeta_{model_name}{zeta_train}_Reg_{num_epochs}_seed{seed}.txt', Acc_train)\n",
    "                np.savetxt(f'results/cifar_watermark/testAcc_zeta_{model_name}_{zeta_train}_Reg_{num_epochs}_seed{seed}.txt', Acc_test)\n",
    "                np.savetxt(f'results/cifar_watermark/clsLoss_zeta_{model_name}_{zeta_train}_Reg_{num_epochs}_seed{seed}.txt', Cls_loss)\n",
    "                np.savetxt(f'results/cifar_watermark/regLoss_zeta{model_name}_{zeta_train}_Reg_{num_epochs}_seed{seed}.txt', Reg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa56a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Seed: 0=============================\n",
      "========================Model: resnet=========================\n",
      "========================Zeta: 0.999===========================\n",
      "S/M/Z [0/resnet/0] Epoch [1/200], grp [0], Loss [0.5121/0.1993], Gropu Acc [0.00,0.00,1.00,1.00] Test: [0.5000]\n",
      "S/M/Z [0/resnet/0] Epoch [2/200], grp [0], Loss [0.4711/0.2737], Gropu Acc [0.00,0.00,1.00,1.00] Test: [0.5000]\n",
      "S/M/Z [0/resnet/0] Epoch [3/200], grp [0], Loss [0.4494/0.1068], Gropu Acc [0.00,0.20,1.00,1.00] Test: [0.5020]\n",
      "S/M/Z [0/resnet/0] Epoch [4/200], grp [0], Loss [0.4506/0.0885], Gropu Acc [0.04,0.60,1.00,0.99] Test: [0.5120]\n",
      "S/M/Z [0/resnet/0] Epoch [5/200], grp [0], Loss [0.4280/0.1913], Gropu Acc [0.17,0.80,1.00,0.97] Test: [0.5640]\n",
      "S/M/Z [0/resnet/0] Epoch [6/200], grp [0], Loss [0.4256/0.1008], Gropu Acc [0.34,1.00,1.00,0.93] Test: [0.6190]\n",
      "S/M/Z [0/resnet/0] Epoch [7/200], grp [0], Loss [0.4092/0.1773], Gropu Acc [0.51,1.00,1.00,0.87] Test: [0.6850]\n",
      "S/M/Z [0/resnet/0] Epoch [8/200], grp [0], Loss [0.3976/0.0000], Gropu Acc [0.62,1.00,1.00,0.83] Test: [0.7210]\n",
      "S/M/Z [0/resnet/0] Epoch [9/200], grp [0], Loss [0.4001/0.1811], Gropu Acc [0.69,1.00,1.00,0.80] Test: [0.7390]\n",
      "S/M/Z [0/resnet/0] Epoch [10/200], grp [0], Loss [0.3802/0.1862], Gropu Acc [0.76,1.00,1.00,0.77] Test: [0.7515]\n",
      "S/M/Z [0/resnet/0] Epoch [11/200], grp [3], Loss [0.3487/0.0000], Gropu Acc [0.83,1.00,1.00,0.73] Test: [0.7590]\n",
      "S/M/Z [0/resnet/0] Epoch [12/200], grp [3], Loss [0.3471/0.1760], Gropu Acc [0.86,1.00,1.00,0.71] Test: [0.7710]\n",
      "S/M/Z [0/resnet/0] Epoch [13/200], grp [3], Loss [0.3352/0.1819], Gropu Acc [0.86,1.00,1.00,0.75] Test: [0.7870]\n",
      "S/M/Z [0/resnet/0] Epoch [14/200], grp [3], Loss [0.3216/0.2755], Gropu Acc [0.86,1.00,1.00,0.77] Test: [0.8035]\n",
      "S/M/Z [0/resnet/0] Epoch [15/200], grp [3], Loss [0.3072/0.1804], Gropu Acc [0.86,1.00,1.00,0.80] Test: [0.8075]\n",
      "S/M/Z [0/resnet/0] Epoch [16/200], grp [3], Loss [0.2952/0.1704], Gropu Acc [0.87,1.00,1.00,0.82] Test: [0.8200]\n",
      "S/M/Z [0/resnet/0] Epoch [17/200], grp [3], Loss [0.2773/0.1814], Gropu Acc [0.87,1.00,1.00,0.83] Test: [0.8265]\n",
      "S/M/Z [0/resnet/0] Epoch [18/200], grp [3], Loss [0.2644/0.0810], Gropu Acc [0.87,1.00,1.00,0.85] Test: [0.8375]\n",
      "S/M/Z [0/resnet/0] Epoch [19/200], grp [3], Loss [0.2537/0.0000], Gropu Acc [0.88,1.00,1.00,0.86] Test: [0.8455]\n",
      "S/M/Z [0/resnet/0] Epoch [20/200], grp [3], Loss [0.2401/0.1662], Gropu Acc [0.88,1.00,1.00,0.88] Test: [0.8555]\n",
      "S/M/Z [0/resnet/0] Epoch [21/200], grp [0], Loss [0.2251/0.2533], Gropu Acc [0.89,1.00,1.00,0.89] Test: [0.8615]\n",
      "S/M/Z [0/resnet/0] Epoch [22/200], grp [0], Loss [0.2102/0.1614], Gropu Acc [0.89,1.00,1.00,0.90] Test: [0.8640]\n",
      "S/M/Z [0/resnet/0] Epoch [23/200], grp [0], Loss [0.1954/0.0000], Gropu Acc [0.90,1.00,1.00,0.91] Test: [0.8675]\n",
      "S/M/Z [0/resnet/0] Epoch [24/200], grp [0], Loss [0.1807/0.1516], Gropu Acc [0.90,1.00,1.00,0.92] Test: [0.8745]\n",
      "S/M/Z [0/resnet/0] Epoch [25/200], grp [0], Loss [0.1662/0.1474], Gropu Acc [0.91,1.00,1.00,0.93] Test: [0.8790]\n",
      "S/M/Z [0/resnet/0] Epoch [26/200], grp [0], Loss [0.1528/0.1561], Gropu Acc [0.92,1.00,1.00,0.93] Test: [0.8860]\n",
      "S/M/Z [0/resnet/0] Epoch [27/200], grp [0], Loss [0.1397/0.1477], Gropu Acc [0.92,1.00,1.00,0.94] Test: [0.8935]\n",
      "S/M/Z [0/resnet/0] Epoch [28/200], grp [0], Loss [0.1280/0.0000], Gropu Acc [0.93,1.00,1.00,0.94] Test: [0.8980]\n",
      "S/M/Z [0/resnet/0] Epoch [29/200], grp [0], Loss [0.1173/0.1478], Gropu Acc [0.93,1.00,1.00,0.95] Test: [0.9020]\n",
      "S/M/Z [0/resnet/0] Epoch [30/200], grp [0], Loss [0.1072/0.1382], Gropu Acc [0.94,1.00,1.00,0.95] Test: [0.9075]\n",
      "S/M/Z [0/resnet/0] Epoch [31/200], grp [0], Loss [0.0983/0.2033], Gropu Acc [0.94,1.00,1.00,0.95] Test: [0.9125]\n",
      "S/M/Z [0/resnet/0] Epoch [32/200], grp [0], Loss [0.0899/0.0672], Gropu Acc [0.95,1.00,1.00,0.95] Test: [0.9120]\n",
      "S/M/Z [0/resnet/0] Epoch [33/200], grp [0], Loss [0.0827/0.0000], Gropu Acc [0.95,1.00,1.00,0.96] Test: [0.9165]\n",
      "S/M/Z [0/resnet/0] Epoch [34/200], grp [0], Loss [0.0759/0.0527], Gropu Acc [0.96,1.00,1.00,0.96] Test: [0.9195]\n",
      "S/M/Z [0/resnet/0] Epoch [35/200], grp [0], Loss [0.0704/0.1174], Gropu Acc [0.96,1.00,1.00,0.96] Test: [0.9220]\n",
      "S/M/Z [0/resnet/0] Epoch [36/200], grp [0], Loss [0.0652/0.0000], Gropu Acc [0.96,1.00,1.00,0.96] Test: [0.9235]\n",
      "S/M/Z [0/resnet/0] Epoch [37/200], grp [3], Loss [0.0607/0.1209], Gropu Acc [0.97,1.00,1.00,0.97] Test: [0.9235]\n",
      "S/M/Z [0/resnet/0] Epoch [38/200], grp [3], Loss [0.0566/0.0590], Gropu Acc [0.97,1.00,1.00,0.97] Test: [0.9240]\n",
      "S/M/Z [0/resnet/0] Epoch [39/200], grp [3], Loss [0.0526/0.0628], Gropu Acc [0.97,1.00,1.00,0.97] Test: [0.9240]\n",
      "S/M/Z [0/resnet/0] Epoch [40/200], grp [3], Loss [0.0488/0.1121], Gropu Acc [0.98,1.00,1.00,0.97] Test: [0.9240]\n",
      "S/M/Z [0/resnet/0] Epoch [41/200], grp [3], Loss [0.0454/0.0536], Gropu Acc [0.98,1.00,1.00,0.98] Test: [0.9250]\n",
      "S/M/Z [0/resnet/0] Epoch [42/200], grp [3], Loss [0.0418/0.0999], Gropu Acc [0.98,1.00,1.00,0.98] Test: [0.9260]\n",
      "S/M/Z [0/resnet/0] Epoch [43/200], grp [3], Loss [0.0397/0.1017], Gropu Acc [0.98,1.00,1.00,0.98] Test: [0.9260]\n",
      "S/M/Z [0/resnet/0] Epoch [44/200], grp [3], Loss [0.0369/0.0000], Gropu Acc [0.98,1.00,1.00,0.98] Test: [0.9260]\n",
      "S/M/Z [0/resnet/0] Epoch [45/200], grp [3], Loss [0.0344/0.0457], Gropu Acc [0.99,1.00,1.00,0.98] Test: [0.9260]\n",
      "S/M/Z [0/resnet/0] Epoch [46/200], grp [3], Loss [0.0319/0.0543], Gropu Acc [0.99,1.00,1.00,0.98] Test: [0.9260]\n",
      "S/M/Z [0/resnet/0] Epoch [47/200], grp [3], Loss [0.0297/0.1617], Gropu Acc [0.99,1.00,1.00,0.98] Test: [0.9255]\n",
      "S/M/Z [0/resnet/0] Epoch [48/200], grp [3], Loss [0.0283/0.0496], Gropu Acc [0.99,1.00,1.00,0.98] Test: [0.9225]\n",
      "S/M/Z [0/resnet/0] Epoch [49/200], grp [3], Loss [0.0264/0.1056], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9230]\n",
      "S/M/Z [0/resnet/0] Epoch [50/200], grp [3], Loss [0.0248/0.1449], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9220]\n",
      "S/M/Z [0/resnet/0] Epoch [51/200], grp [3], Loss [0.0230/0.1476], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9215]\n",
      "S/M/Z [0/resnet/0] Epoch [52/200], grp [3], Loss [0.0219/0.1478], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9220]\n",
      "S/M/Z [0/resnet/0] Epoch [53/200], grp [3], Loss [0.0202/0.0886], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9225]\n",
      "S/M/Z [0/resnet/0] Epoch [54/200], grp [3], Loss [0.0193/0.0985], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9235]\n",
      "S/M/Z [0/resnet/0] Epoch [55/200], grp [3], Loss [0.0178/0.1027], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9220]\n",
      "S/M/Z [0/resnet/0] Epoch [56/200], grp [3], Loss [0.0167/0.0539], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9210]\n",
      "S/M/Z [0/resnet/0] Epoch [57/200], grp [3], Loss [0.0153/0.0968], Gropu Acc [0.99,1.00,1.00,0.99] Test: [0.9195]\n",
      "S/M/Z [0/resnet/0] Epoch [58/200], grp [3], Loss [0.0151/0.0968], Gropu Acc [1.00,1.00,1.00,0.99] Test: [0.9195]\n",
      "S/M/Z [0/resnet/0] Epoch [59/200], grp [3], Loss [0.0137/0.1418], Gropu Acc [1.00,1.00,1.00,0.99] Test: [0.9190]\n",
      "S/M/Z [0/resnet/0] Epoch [60/200], grp [0], Loss [0.0134/0.1188], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9185]\n",
      "S/M/Z [0/resnet/0] Epoch [61/200], grp [3], Loss [0.0124/0.0000], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9190]\n",
      "S/M/Z [0/resnet/0] Epoch [62/200], grp [3], Loss [0.0115/0.0485], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9190]\n",
      "S/M/Z [0/resnet/0] Epoch [63/200], grp [3], Loss [0.0109/0.0941], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9190]\n",
      "S/M/Z [0/resnet/0] Epoch [64/200], grp [0], Loss [0.0103/0.0909], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9180]\n",
      "S/M/Z [0/resnet/0] Epoch [65/200], grp [0], Loss [0.0098/0.0000], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9180]\n",
      "S/M/Z [0/resnet/0] Epoch [66/200], grp [0], Loss [0.0091/0.0879], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9180]\n",
      "S/M/Z [0/resnet/0] Epoch [67/200], grp [0], Loss [0.0087/0.1263], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9180]\n",
      "S/M/Z [0/resnet/0] Epoch [68/200], grp [3], Loss [0.0086/0.0814], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9175]\n",
      "S/M/Z [0/resnet/0] Epoch [69/200], grp [3], Loss [0.0077/0.0815], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9160]\n",
      "S/M/Z [0/resnet/0] Epoch [70/200], grp [3], Loss [0.0074/0.1389], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9165]\n",
      "S/M/Z [0/resnet/0] Epoch [71/200], grp [3], Loss [0.0071/0.0431], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S/M/Z [0/resnet/0] Epoch [72/200], grp [3], Loss [0.0067/0.1169], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9155]\n",
      "S/M/Z [0/resnet/0] Epoch [73/200], grp [3], Loss [0.0064/0.0885], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9160]\n",
      "S/M/Z [0/resnet/0] Epoch [74/200], grp [3], Loss [0.0060/0.1242], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9160]\n",
      "S/M/Z [0/resnet/0] Epoch [75/200], grp [3], Loss [0.0057/0.0897], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9165]\n",
      "S/M/Z [0/resnet/0] Epoch [76/200], grp [3], Loss [0.0055/0.0436], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9155]\n",
      "S/M/Z [0/resnet/0] Epoch [77/200], grp [3], Loss [0.0054/0.0000], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [78/200], grp [3], Loss [0.0052/0.0830], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9160]\n",
      "S/M/Z [0/resnet/0] Epoch [79/200], grp [3], Loss [0.0049/0.0782], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9160]\n",
      "S/M/Z [0/resnet/0] Epoch [80/200], grp [0], Loss [0.0047/0.0457], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9155]\n",
      "S/M/Z [0/resnet/0] Epoch [81/200], grp [3], Loss [0.0044/0.1002], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [82/200], grp [3], Loss [0.0043/0.0373], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [83/200], grp [3], Loss [0.0042/0.0941], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [84/200], grp [3], Loss [0.0039/0.0000], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [85/200], grp [3], Loss [0.0039/0.1273], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [86/200], grp [3], Loss [0.0036/0.0770], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [87/200], grp [3], Loss [0.0035/0.0786], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [88/200], grp [3], Loss [0.0033/0.0000], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [89/200], grp [3], Loss [0.0033/0.0460], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9140]\n",
      "S/M/Z [0/resnet/0] Epoch [90/200], grp [3], Loss [0.0032/0.0400], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [91/200], grp [3], Loss [0.0031/0.0454], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [92/200], grp [3], Loss [0.0032/0.1280], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [93/200], grp [3], Loss [0.0030/0.0405], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [94/200], grp [3], Loss [0.0027/0.0898], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [95/200], grp [3], Loss [0.0026/0.0744], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [96/200], grp [0], Loss [0.0026/0.0886], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [97/200], grp [0], Loss [0.0026/0.0451], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9135]\n",
      "S/M/Z [0/resnet/0] Epoch [98/200], grp [0], Loss [0.0025/0.1266], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [99/200], grp [0], Loss [0.0025/0.0721], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9145]\n",
      "S/M/Z [0/resnet/0] Epoch [100/200], grp [0], Loss [0.0024/0.0000], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9140]\n",
      "S/M/Z [0/resnet/0] Epoch [101/200], grp [0], Loss [0.0022/0.0738], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n",
      "S/M/Z [0/resnet/0] Epoch [102/200], grp [0], Loss [0.0023/0.0807], Gropu Acc [1.00,1.00,1.00,1.00] Test: [0.9150]\n"
     ]
    }
   ],
   "source": [
    "if task == \"groupDRO\":\n",
    "    \n",
    "    criterion = nn.BCELoss(reduction = 'none')\n",
    "    def group_accuracy(model, trainSubloader):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            Acc = []\n",
    "            for subloader in trainSubloader:\n",
    "                acc = 0\n",
    "                ct = 0\n",
    "                for data in subloader:\n",
    "                    images, label = data\n",
    "                    images = images.permute(0,3,1,2).to(device)\n",
    "                    label = label[:,0].unsqueeze(1).to(device)\n",
    "                    _, pred = model(images)\n",
    "                    acc += ((pred > 0.5)==label).float().sum().item()\n",
    "                    ct += len(images)\n",
    "                Acc.append(acc/ct)\n",
    "        return Acc\n",
    "\n",
    "    def total_accuracy(model, dataloader):\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        ct = 0\n",
    "        for data in dataloader:\n",
    "            images, label = data\n",
    "            images = images.permute(0,3,1,2).to(device)\n",
    "            label = label[:,0].unsqueeze(1).to(device)\n",
    "            _, pred = model(images)\n",
    "            acc += ((pred > 0.5)==label).float().sum().item()\n",
    "            ct += len(images)\n",
    "        return acc/ct\n",
    "\n",
    "\n",
    "    def train(model, dataloader):\n",
    "        model.train()\n",
    "        cls_loss = 0\n",
    "        reg_loss = 0\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            images, target = data\n",
    "            images = images.permute(0,3,1,2).to(device)\n",
    "            label = target[:,0].unsqueeze(1).to(device)\n",
    "            grp = torch.tensor([group_mapping[(target[i,0].item(), target[i,1].item())] for i in range(len(target))])\n",
    "            grp = grp.to(device)\n",
    "\n",
    "            emb, pred = model(images)\n",
    "            loss_cls = loss_fn(pred, label, grp)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if ((data[1][:,1] == 1)*(data[1][:,0] == 0)).sum().item()*((data[1][:,1] == 0)*(data[1][:,0] == 1)).sum().item() == 0:\n",
    "                    loss_reg = 0\n",
    "                else:\n",
    "                    loss_reg = reg(emb,images, data[1][:,0], data[1][:,1])\n",
    "\n",
    "            loss = loss_cls\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cls_loss += loss_cls.item()\n",
    "            if loss_reg>0:\n",
    "                reg_loss += loss_reg.item()\n",
    "        return cls_loss, reg_loss\n",
    "\n",
    "    group_mapping = {\n",
    "        (1,1): 0, (1,0): 1, (0,1): 2, (0,0):3\n",
    "    }\n",
    "\n",
    "    def compute_grp_avg(losses, grp):\n",
    "        grp_map = (grp == torch.arange(4).unsqueeze(1).long().to(losses.device)).float()\n",
    "        grp_count = grp_map.sum(1)\n",
    "        grp_denom = grp_count + (grp_count==0).float() # avoid nans\n",
    "        grp_loss = (grp_map @ losses.view(-1))/grp_denom\n",
    "        return grp_loss, grp_count\n",
    "\n",
    "    def compute_robust_loss(grp_loss, grp_count, step_size = 1e-2):\n",
    "        adv_probs = torch.ones(4).to(grp_loss.device)/4\n",
    "        adjusted_loss = grp_loss\n",
    "        adv_probs = torch.exp(step_size*adjusted_loss.data)\n",
    "        adv_probs = adv_probs/(adv_probs.sum())\n",
    "        robust_loss = grp_loss @ adv_probs\n",
    "        return robust_loss, adv_probs\n",
    "\n",
    "    def loss_fn(pred, label, grp):\n",
    "        per_sample_losses = criterion(pred, label)\n",
    "        grp_loss, grp_count = compute_grp_avg(per_sample_losses, grp)\n",
    "        grp_acc, grp_count = compute_grp_avg(((pred > 0.5) == label).float(), grp)\n",
    "        actual_loss, weights = compute_robust_loss(grp_loss, grp_count)\n",
    "        return actual_loss\n",
    "\n",
    "    \n",
    "    for seed in range(10):\n",
    "        print(f'==========================Seed: {seed}=============================')\n",
    "        for model_name in ['resnet', 'vgg', 'alexnet']:\n",
    "            print(f'========================Model: {model_name}=========================')\n",
    "            for zeta_train in zeta_trains:   \n",
    "                print(f'========================Zeta: {zeta_train}===========================')\n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                \n",
    "                testset_nonbiased = CIFAR_subset_watermark(testset, index_test, alpha = .5, beta = .5)\n",
    "                trainset_biased = CIFAR_subset_watermark(trainset, index_train, alpha = zeta_train, beta = zeta_train)\n",
    "\n",
    "                trainloader = DataLoader(trainset_biased, batch_size=batch_size, num_workers = 8, shuffle=True)\n",
    "                testloader = DataLoader(testset_nonbiased, batch_size=batch_size, num_workers = 8, shuffle=False)\n",
    "\n",
    "\n",
    "                indices_11 = np.where((trainset_biased.targets == 1) * (trainset_biased.spurious == 1))[0]\n",
    "                indices_10 = np.where((trainset_biased.targets == 1) * (trainset_biased.spurious == 0))[0]\n",
    "                indices_01 = np.where((trainset_biased.targets == 0) * (trainset_biased.spurious == 1))[0]\n",
    "                indices_00 = np.where((trainset_biased.targets == 0) * (trainset_biased.spurious == 0))[0]\n",
    "                trainSubloader = [\n",
    "                    DataLoader(torch.utils.data.Subset(trainset_biased, indices = indices_11),\n",
    "                               batch_size=batch_size, num_workers = 8, shuffle=True),\n",
    "                    DataLoader(torch.utils.data.Subset(trainset_biased, indices = indices_10),\n",
    "                               batch_size=batch_size, num_workers = 8, shuffle=True),\n",
    "                    DataLoader(torch.utils.data.Subset(trainset_biased, indices = indices_01),\n",
    "                               batch_size=batch_size, num_workers = 8, shuffle=True),\n",
    "                    DataLoader(torch.utils.data.Subset(trainset_biased, indices = indices_00),\n",
    "                               batch_size=batch_size, num_workers = 8, shuffle=True)\n",
    "                ]\n",
    "\n",
    "                if model_name == 'resnet':\n",
    "                    model = Classifier_resnet(latent_dim).to(device)\n",
    "                elif model_name == 'vgg':\n",
    "                    model = Classifier_vgg(latent_dim).to(device)\n",
    "                elif model_name == 'alexnet':\n",
    "                    model = Classifier_alexnet(latent_dim).to(device)\n",
    "                    \n",
    "                optimizer = optim.SGD(model.parameters(), lr = 1e-3, momentum = 0.9, weight_decay = 5e-4)\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer,'min',factor=0.1,patience=5,threshold=0.0001,min_lr=0,eps=1e-08)\n",
    "                Acc_train = []\n",
    "                Acc_test = []\n",
    "                Cls_loss = []\n",
    "                Reg_loss = []\n",
    "                for epoch in range(num_epochs):\n",
    "                    model.eval()\n",
    "\n",
    "                    cls_loss, reg_loss = train(model, trainloader)\n",
    "                    Acc = group_accuracy(model, trainSubloader)\n",
    "                    scheduler.step(cls_loss)\n",
    "                    worst_group = Acc.index(min(Acc))\n",
    "                    acc_test = total_accuracy(model, testloader)\n",
    "\n",
    "                    print('S/M/Z [%d/%s/%d] Epoch [%d/%d], grp [%d], Loss [%.4f/%.4f], Gropu Acc [%.2f,%.2f,%.2f,%.2f] Test: [%.4f]' % (\n",
    "                        seed, model_name, zeta_train, epoch+1, num_epochs, worst_group, cls_loss/len(trainloader), reg_loss/len(trainloader),\n",
    "                        Acc[0], Acc[1], Acc[2], Acc[3], acc_test))\n",
    "\n",
    "                    Acc_train.append(Acc)\n",
    "                    Acc_test.append(acc_test)\n",
    "                    Cls_loss.append(cls_loss/len(trainloader))\n",
    "                    Reg_loss.append(reg_loss/len(trainloader))\n",
    "\n",
    "                torch.save(model.state_dict(), f'results/cifar_watermark/DRO_{model_name}_{zeta_train}_{num_epochs}_seed{seed}')\n",
    "                np.savetxt(f'results/cifar_watermark/trainAcc_zeta_{model_name}{zeta_train}_DRO_{num_epochs}_seed{seed}.txt', Acc_train)\n",
    "                np.savetxt(f'results/cifar_watermark/testAcc_zeta_{model_name}_{zeta_train}_DRO_{num_epochs}_seed{seed}.txt', Acc_test)\n",
    "                np.savetxt(f'results/cifar_watermark/clsLoss_zeta_{model_name}_{zeta_train}_DRO_{num_epochs}_seed{seed}.txt', Cls_loss)\n",
    "                np.savetxt(f'results/cifar_watermark/regLoss_zeta{model_name}_{zeta_train}_DRO_{num_epochs}_seed{seed}.txt', Reg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576e824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec7ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
